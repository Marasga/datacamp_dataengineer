{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Datacamp - streamlined dataingestion with pandas</h1>\n",
    "<h3>1 - Importing data from flat files</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Datafames -> struttura specifica per dati bi-dimensionali.\n",
    "\n",
    "Possiedono delle etichette di riga (row labels) detta indice (index) e delle etichette per le colonne (column labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pandas supporta più tipi di dati per l'importazione, il più semplice è il flat file (.csv è un esempio di flat file).\n",
    "La funzione pandas per leggere i flat file è la read_csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Name</th>\n",
       "      <th>Job Title</th>\n",
       "      <th>Address</th>\n",
       "      <th>State</th>\n",
       "      <th>City</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>John Doe</td>\n",
       "      <td>Designer</td>\n",
       "      <td>325 Pine Street</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Seattle</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Edward Green</td>\n",
       "      <td>Developer</td>\n",
       "      <td>110 Pike Street</td>\n",
       "      <td>WA</td>\n",
       "      <td>Seattle</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           Name  Job Title          Address State     City\n",
       "0      John Doe   Designer  325 Pine Street   NaN  Seattle\n",
       "1           NaN        NaN              NaN   NaN      NaN\n",
       "2  Edward Green  Developer  110 Pike Street    WA  Seattle"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# importare la libreiria pandas che contiene gli oggetti dataframe e i metodi associati\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Definire in unav variabile stringa il path del flat file da importare\n",
    "data_path = \"Data/example.csv\"\n",
    "\n",
    "# La funzione di pandas \"read_csv\" permette fornito il path di importare il csv in un dataframe\n",
    "#la funzione accetta alcuni parametri come \"sep\" che permette di impostare il separatore\n",
    "example_data = pd.read_csv(\"Data/example.csv\")\n",
    "\n",
    "# I metodi \"head\" e \"tail\" permettono di vedere l'inizio e la fine dei dataframe\n",
    "example_data.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>type</th>\n",
       "      <th>submitter_id</th>\n",
       "      <th>projects.code</th>\n",
       "      <th>data_description</th>\n",
       "      <th>study_description</th>\n",
       "      <th>study_design</th>\n",
       "      <th>study_objective</th>\n",
       "      <th>study_setup</th>\n",
       "      <th>associated_study</th>\n",
       "      <th>project_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>study</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    type  submitter_id  projects.code  data_description  study_description  \\\n",
       "0  study           NaN            NaN               NaN                NaN   \n",
       "\n",
       "   study_design  study_objective  study_setup  associated_study  project_id  \n",
       "0           NaN              NaN          NaN               NaN         NaN  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Tramite i parametri è possibile importare file che utilizzano altri separatori\n",
    "\n",
    "tsv_example = pd.read_csv(\"Data/study.tsv\", sep=\"\\t\")\n",
    "\n",
    "tsv_example.tail(5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h6> 1a - Modifyng flat file imports </h6>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7787, 12)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_path = \"Data/netflix_titles.csv\"\n",
    "\n",
    "df = pd.read_csv(data_path)\n",
    "\n",
    "# la proprietà shape di un dataframe ci permette di espicitare la dimensione del dataframe in termini\n",
    "# di righe e colonne\n",
    "\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7787, 2)\n",
      "(7787, 2)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>type</th>\n",
       "      <th>title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>7782</th>\n",
       "      <td>Movie</td>\n",
       "      <td>Zozo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7783</th>\n",
       "      <td>Movie</td>\n",
       "      <td>Zubaan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7784</th>\n",
       "      <td>Movie</td>\n",
       "      <td>Zulu Man in Japan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7785</th>\n",
       "      <td>TV Show</td>\n",
       "      <td>Zumbo's Just Desserts</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7786</th>\n",
       "      <td>Movie</td>\n",
       "      <td>ZZ TOP: THAT LITTLE OL' BAND FROM TEXAS</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         type                                    title\n",
       "7782    Movie                                     Zozo\n",
       "7783    Movie                                   Zubaan\n",
       "7784    Movie                        Zulu Man in Japan\n",
       "7785  TV Show                    Zumbo's Just Desserts\n",
       "7786    Movie  ZZ TOP: THAT LITTLE OL' BAND FROM TEXAS"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# per ridurre il numero di dati che vengono caricati una possibilità è usare il parametro \"usecols\"\n",
    "# il parametro accetta una lista di numeri di colonne o nomi oppure anche una fulzione che filtri le colonne\n",
    "\n",
    "df_selected = pd.read_csv(data_path, usecols=(1,2))\n",
    "\n",
    "print(df_selected.shape)\n",
    "\n",
    "col_names = ['type','title']\n",
    "\n",
    "df_selected = pd.read_csv(data_path, usecols=col_names)\n",
    "\n",
    "print(df_selected.shape)\n",
    "\n",
    "df_selected.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000, 12)\n"
     ]
    }
   ],
   "source": [
    "# E' possibile in alternativa o allo stesso tempo limitare il numero di righe che del file che vengono caricate\n",
    "# tramite la keywork \"nrows\"\n",
    "\n",
    "df_selected_rows = pd.read_csv(data_path, nrows=1000)\n",
    "\n",
    "print(df_selected_rows.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3893, 12)\n"
     ]
    }
   ],
   "source": [
    "# il parametro \"skiprows\" accetta una lista di numeri di riga, un numero di righe o una funzione per filtrare\n",
    "# le righe\n",
    "\n",
    "df_selected_rows = pd.read_csv(data_path, skiprows=(lambda x:x%2==0))\n",
    "\n",
    "# nel'esempio abbiamo utilizzato una funzione lambda per prendere solo le righe dispari\n",
    "print(df_selected_rows.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>s1</td>\n",
       "      <td>TV Show</td>\n",
       "      <td>3%</td>\n",
       "      <td>NaN</td>\n",
       "      <td>João Miguel, Bianca Comparato, Michel Gomes, R...</td>\n",
       "      <td>Brazil</td>\n",
       "      <td>August 14, 2020</td>\n",
       "      <td>2020</td>\n",
       "      <td>TV-MA</td>\n",
       "      <td>4 Seasons</td>\n",
       "      <td>International TV Shows, TV Dramas, TV Sci-Fi &amp;...</td>\n",
       "      <td>In a future where the elite inhabit an island ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>s3</td>\n",
       "      <td>Movie</td>\n",
       "      <td>23:59</td>\n",
       "      <td>Gilbert Chan</td>\n",
       "      <td>Tedd Chan, Stella Chung, Henley Hii, Lawrence ...</td>\n",
       "      <td>Singapore</td>\n",
       "      <td>December 20, 2018</td>\n",
       "      <td>2011</td>\n",
       "      <td>R</td>\n",
       "      <td>78 min</td>\n",
       "      <td>Horror Movies, International Movies</td>\n",
       "      <td>When an army recruit is found dead, his fellow...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>s5</td>\n",
       "      <td>Movie</td>\n",
       "      <td>21</td>\n",
       "      <td>Robert Luketic</td>\n",
       "      <td>Jim Sturgess, Kevin Spacey, Kate Bosworth, Aar...</td>\n",
       "      <td>United States</td>\n",
       "      <td>January 1, 2020</td>\n",
       "      <td>2008</td>\n",
       "      <td>PG-13</td>\n",
       "      <td>123 min</td>\n",
       "      <td>Dramas</td>\n",
       "      <td>A brilliant group of students become card-coun...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>s7</td>\n",
       "      <td>Movie</td>\n",
       "      <td>122</td>\n",
       "      <td>Yasir Al Yasiri</td>\n",
       "      <td>Amina Khalil, Ahmed Dawood, Tarek Lotfy, Ahmed...</td>\n",
       "      <td>Egypt</td>\n",
       "      <td>June 1, 2020</td>\n",
       "      <td>2019</td>\n",
       "      <td>TV-MA</td>\n",
       "      <td>95 min</td>\n",
       "      <td>Horror Movies, International Movies</td>\n",
       "      <td>After an awful accident, a couple admitted to ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>s9</td>\n",
       "      <td>Movie</td>\n",
       "      <td>706</td>\n",
       "      <td>Shravan Kumar</td>\n",
       "      <td>Divya Dutta, Atul Kulkarni, Mohan Agashe, Anup...</td>\n",
       "      <td>India</td>\n",
       "      <td>April 1, 2019</td>\n",
       "      <td>2019</td>\n",
       "      <td>TV-14</td>\n",
       "      <td>118 min</td>\n",
       "      <td>Horror Movies, International Movies</td>\n",
       "      <td>When a doctor goes missing, his psychiatrist w...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   0        1      2                3   \\\n",
       "0  s1  TV Show     3%              NaN   \n",
       "1  s3    Movie  23:59     Gilbert Chan   \n",
       "2  s5    Movie     21   Robert Luketic   \n",
       "3  s7    Movie    122  Yasir Al Yasiri   \n",
       "4  s9    Movie    706    Shravan Kumar   \n",
       "\n",
       "                                                  4              5   \\\n",
       "0  João Miguel, Bianca Comparato, Michel Gomes, R...         Brazil   \n",
       "1  Tedd Chan, Stella Chung, Henley Hii, Lawrence ...      Singapore   \n",
       "2  Jim Sturgess, Kevin Spacey, Kate Bosworth, Aar...  United States   \n",
       "3  Amina Khalil, Ahmed Dawood, Tarek Lotfy, Ahmed...          Egypt   \n",
       "4  Divya Dutta, Atul Kulkarni, Mohan Agashe, Anup...          India   \n",
       "\n",
       "                  6     7      8          9   \\\n",
       "0    August 14, 2020  2020  TV-MA  4 Seasons   \n",
       "1  December 20, 2018  2011      R     78 min   \n",
       "2    January 1, 2020  2008  PG-13    123 min   \n",
       "3       June 1, 2020  2019  TV-MA     95 min   \n",
       "4      April 1, 2019  2019  TV-14    118 min   \n",
       "\n",
       "                                                  10  \\\n",
       "0  International TV Shows, TV Dramas, TV Sci-Fi &...   \n",
       "1                Horror Movies, International Movies   \n",
       "2                                             Dramas   \n",
       "3                Horror Movies, International Movies   \n",
       "4                Horror Movies, International Movies   \n",
       "\n",
       "                                                  11  \n",
       "0  In a future where the elite inhabit an island ...  \n",
       "1  When an army recruit is found dead, his fellow...  \n",
       "2  A brilliant group of students become card-coun...  \n",
       "3  After an awful accident, a couple admitted to ...  \n",
       "4  When a doctor goes missing, his psychiatrist w...  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_selected_rows.head()\n",
    "\n",
    "# attenzione, se si skippa la riga contenente i nomi delle colonne è importante settare il parametro \"header\"\n",
    "# = None per esplicitare il fatto che nel nostro file non sono presenti gli header\n",
    "\n",
    "df_selected_rows = pd.read_csv(data_path, skiprows=(lambda x:x%2==0), header=None)\n",
    "df_selected_rows.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>show_id</th>\n",
       "      <th>type</th>\n",
       "      <th>title</th>\n",
       "      <th>director</th>\n",
       "      <th>cast</th>\n",
       "      <th>country</th>\n",
       "      <th>date_added</th>\n",
       "      <th>release_year</th>\n",
       "      <th>rating</th>\n",
       "      <th>duration</th>\n",
       "      <th>listed_in</th>\n",
       "      <th>description</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>s1</td>\n",
       "      <td>TV Show</td>\n",
       "      <td>3%</td>\n",
       "      <td>NaN</td>\n",
       "      <td>João Miguel, Bianca Comparato, Michel Gomes, R...</td>\n",
       "      <td>Brazil</td>\n",
       "      <td>August 14, 2020</td>\n",
       "      <td>2020</td>\n",
       "      <td>TV-MA</td>\n",
       "      <td>4 Seasons</td>\n",
       "      <td>International TV Shows, TV Dramas, TV Sci-Fi &amp;...</td>\n",
       "      <td>In a future where the elite inhabit an island ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>s3</td>\n",
       "      <td>Movie</td>\n",
       "      <td>23:59</td>\n",
       "      <td>Gilbert Chan</td>\n",
       "      <td>Tedd Chan, Stella Chung, Henley Hii, Lawrence ...</td>\n",
       "      <td>Singapore</td>\n",
       "      <td>December 20, 2018</td>\n",
       "      <td>2011</td>\n",
       "      <td>R</td>\n",
       "      <td>78 min</td>\n",
       "      <td>Horror Movies, International Movies</td>\n",
       "      <td>When an army recruit is found dead, his fellow...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>s5</td>\n",
       "      <td>Movie</td>\n",
       "      <td>21</td>\n",
       "      <td>Robert Luketic</td>\n",
       "      <td>Jim Sturgess, Kevin Spacey, Kate Bosworth, Aar...</td>\n",
       "      <td>United States</td>\n",
       "      <td>January 1, 2020</td>\n",
       "      <td>2008</td>\n",
       "      <td>PG-13</td>\n",
       "      <td>123 min</td>\n",
       "      <td>Dramas</td>\n",
       "      <td>A brilliant group of students become card-coun...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>s7</td>\n",
       "      <td>Movie</td>\n",
       "      <td>122</td>\n",
       "      <td>Yasir Al Yasiri</td>\n",
       "      <td>Amina Khalil, Ahmed Dawood, Tarek Lotfy, Ahmed...</td>\n",
       "      <td>Egypt</td>\n",
       "      <td>June 1, 2020</td>\n",
       "      <td>2019</td>\n",
       "      <td>TV-MA</td>\n",
       "      <td>95 min</td>\n",
       "      <td>Horror Movies, International Movies</td>\n",
       "      <td>After an awful accident, a couple admitted to ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>s9</td>\n",
       "      <td>Movie</td>\n",
       "      <td>706</td>\n",
       "      <td>Shravan Kumar</td>\n",
       "      <td>Divya Dutta, Atul Kulkarni, Mohan Agashe, Anup...</td>\n",
       "      <td>India</td>\n",
       "      <td>April 1, 2019</td>\n",
       "      <td>2019</td>\n",
       "      <td>TV-14</td>\n",
       "      <td>118 min</td>\n",
       "      <td>Horror Movies, International Movies</td>\n",
       "      <td>When a doctor goes missing, his psychiatrist w...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  show_id     type  title         director  \\\n",
       "0      s1  TV Show     3%              NaN   \n",
       "1      s3    Movie  23:59     Gilbert Chan   \n",
       "2      s5    Movie     21   Robert Luketic   \n",
       "3      s7    Movie    122  Yasir Al Yasiri   \n",
       "4      s9    Movie    706    Shravan Kumar   \n",
       "\n",
       "                                                cast        country  \\\n",
       "0  João Miguel, Bianca Comparato, Michel Gomes, R...         Brazil   \n",
       "1  Tedd Chan, Stella Chung, Henley Hii, Lawrence ...      Singapore   \n",
       "2  Jim Sturgess, Kevin Spacey, Kate Bosworth, Aar...  United States   \n",
       "3  Amina Khalil, Ahmed Dawood, Tarek Lotfy, Ahmed...          Egypt   \n",
       "4  Divya Dutta, Atul Kulkarni, Mohan Agashe, Anup...          India   \n",
       "\n",
       "          date_added  release_year rating   duration  \\\n",
       "0    August 14, 2020          2020  TV-MA  4 Seasons   \n",
       "1  December 20, 2018          2011      R     78 min   \n",
       "2    January 1, 2020          2008  PG-13    123 min   \n",
       "3       June 1, 2020          2019  TV-MA     95 min   \n",
       "4      April 1, 2019          2019  TV-14    118 min   \n",
       "\n",
       "                                           listed_in  \\\n",
       "0  International TV Shows, TV Dramas, TV Sci-Fi &...   \n",
       "1                Horror Movies, International Movies   \n",
       "2                                             Dramas   \n",
       "3                Horror Movies, International Movies   \n",
       "4                Horror Movies, International Movies   \n",
       "\n",
       "                                         description  \n",
       "0  In a future where the elite inhabit an island ...  \n",
       "1  When an army recruit is found dead, his fellow...  \n",
       "2  A brilliant group of students become card-coun...  \n",
       "3  After an awful accident, a couple admitted to ...  \n",
       "4  When a doctor goes missing, his psychiatrist w...  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Quando non è presente l'header è possibile assegnare direttamente i nomi alle colonne tramete il parametro \n",
    "# \"names\", il parametro deve contenere i nomi di TUTTE le colonne presenti ai quali vogliamo assegnare\n",
    "\n",
    "# estraimo dal df i nomi delle colonne inserendoli in una lista\n",
    "col_names = pd.read_csv(data_path).columns\n",
    "\n",
    "# specifichiamo nell'argomento dedicato la lista\n",
    "df_selected_rows = pd.read_csv(data_path, skiprows=(lambda x:x%2==0), header=None, names=col_names)\n",
    "df_selected_rows.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h6> 1b - Handling errors and missing data </h6>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Errori tipici presenti in un dataset:\n",
    "* data type di colonne sbagliati\n",
    "* Valori mancanti\n",
    "* Record che non possono essere letti"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "show_id         object\n",
      "type            object\n",
      "title           object\n",
      "director        object\n",
      "cast            object\n",
      "country         object\n",
      "date_added      object\n",
      "release_year     int64\n",
      "rating          object\n",
      "duration        object\n",
      "listed_in       object\n",
      "description     object\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "# Pandas tipicamente quando si legge da un flat file se non specificato altrementi fa l'infer del tipo dato\n",
    "# ovvero deduce quale tipo dato assegnare alla colonna\n",
    "\n",
    "# utilizzando l'argomento \"dtype\" è possibile specificare i data type delle colonne, questo parametro riceve\n",
    "# come argomento un dizionario dove ogni chiave è il nome di una colonna e ogni valore è il tipo dato\n",
    "\n",
    "\n",
    "data_t = {\"show_id\":str,\"type\":str}#,\"date_added\":np.datetime64}\n",
    "\n",
    "df = pd.read_csv(data_path,dtype=data_t)\n",
    "\n",
    "print(df.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1169, 81)\n"
     ]
    }
   ],
   "source": [
    "# pandas interpreta i valori nulli come Na, tuttavia è possibile specificare laddove troviamo dei dati non\n",
    "# compatibili che pandas li interpreti come Na tramite il parametro \"na_values\", questi può ricevere un \n",
    "# singolo valore, una lista o un  dizionario di colonne e valori\n",
    "data_path = \"Data/missing_data.csv\"\n",
    "\n",
    "df_missing = pd.read_csv(data_path)\n",
    "\n",
    "# il metodo isna può essere ultilzzato per individuare in tutti i record del dataframe dove sono presenti valori \n",
    "# na\n",
    "\n",
    "df_missing.Fence.isna().head()\n",
    "\n",
    "# es: filtrare la porzione del df che contiene valori na nella colonna Fence\n",
    "\n",
    "df_filtered = df_missing[df_missing.Fence.isna()]\n",
    "\n",
    "print(df_filtered.shape)\n",
    "# il risultato ci mostra quante sono le righe che hanno "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'pandas.io.common' has no attribute 'CParserError'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-a8bd24070ece>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m   \u001b[0;31m# Set warn_bad_lines to issue warnings about bad records\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m   data = pd.read_csv(\"vt_tax_data_2016_corrupt.csv\", \n\u001b[0m\u001b[1;32m      9\u001b[0m                      \u001b[0merror_bad_lines\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.9/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[1;32m    609\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 610\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    611\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.9/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    461\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 462\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    463\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.9/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    818\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 819\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    820\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.9/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m   1049\u001b[0m         \u001b[0;31m# error: Too many arguments for \"ParserBase\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1050\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mmapping\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1051\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.9/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m   1866\u001b[0m         \u001b[0;31m# open handles\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1867\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_open_handles\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1868\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.9/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_open_handles\u001b[0;34m(self, src, kwds)\u001b[0m\n\u001b[1;32m   1361\u001b[0m         \"\"\"\n\u001b[0;32m-> 1362\u001b[0;31m         self.handles = get_handle(\n\u001b[0m\u001b[1;32m   1363\u001b[0m             \u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.9/site-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    641\u001b[0m             \u001b[0;31m# Encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 642\u001b[0;31m             handle = open(\n\u001b[0m\u001b[1;32m    643\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'vt_tax_data_2016_corrupt.csv'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-a8bd24070ece>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     13\u001b[0m   \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m \u001b[0;32mexcept\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcommon\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCParserError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Your data contained rows that could not be parsed.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: module 'pandas.io.common' has no attribute 'CParserError'"
     ]
    }
   ],
   "source": [
    "# gestire le line errate, ovvero quelle che hanno per esempio più record di tutte le altre\n",
    "# il parametro error_bad_lines permette di settare come vengono gestiti record non elaborabili\n",
    "# error_bad_lines=False indica a pandas di skippare i record corrotti\n",
    "# warn_bad_lines=True farà si che venga lanciato un warning qual'ora vengano skippati dei record\n",
    "\n",
    "try:\n",
    "  # Set warn_bad_lines to issue warnings about bad records\n",
    "  data = pd.read_csv(\"vt_tax_data_2016_corrupt.csv\", \n",
    "                     error_bad_lines=False, \n",
    "                     warn_bad_lines=True)\n",
    "  \n",
    "  # View first 5 records\n",
    "  print(data.head())\n",
    "  \n",
    "except pd.io.common.CParserError:\n",
    "    print(\"Your data contained rows that could not be parsed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> 2 - Importing data from excel files </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['first_name', 'company_name', 'address'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "# Il metodo pandas per importare file excel \"read_excel\"\n",
    "import pandas as pd\n",
    "\n",
    "data_path = \"Data/excel_data.xlsx\"\n",
    "\n",
    "# allo stesso modo di read_csv il motodo possiede alcuni kwargs per aiutare nell'importazione dei fogli di\n",
    "# calcolo: nrows (determina il numero di righe che vengono lette), skiprows (specfica le righe o il numero di\n",
    "# righe da skippare), usecols (selezionare le colonne da importare)\n",
    "\n",
    "# Create string of lettered columns to load\n",
    "col_string =\"A,C:D\"\n",
    "# per i file di tipo excel questo è il formato con il quale indicare quali colonne devono essere caricate\n",
    "# tramite l'uso del parametro usecols\n",
    "\n",
    "# Load data with skiprows and usecols set\n",
    "survey_responses = pd.read_excel(data_path, \n",
    "                        #skiprows=2, \n",
    "                        usecols=col_string)\n",
    "\n",
    "# View the names of the columns selected\n",
    "print(survey_responses.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'dict'>\n",
      "uk-500 <class 'pandas.core.frame.DataFrame'> (2000, 10)\n",
      "Foglio2 <class 'pandas.core.frame.DataFrame'> (2000, 10)\n"
     ]
    }
   ],
   "source": [
    "# E' possibile leggere dei dati da spreadsheet multiplie da più fogli di uno stesso spreadsheet\n",
    "# di default \"read_excel\" , questo è possibile tramite il parametro \"sheet_name\", questo accetta come\n",
    "# argomento: il numero del foglio (position index), il titolo del foglio, una lista di numeri o di stringhe\n",
    "# per ottenere tutti i fogli presenti in uno spreadsheet è possbile passare il valore None al parametro\n",
    "\n",
    "all_sheets = pd.read_excel(data_path, sheet_name=None)\n",
    "\n",
    "# Quando vengono importati più fogli non viene restituito un dataframe da un dizionario ordinato\n",
    "# le chiavi di questo dizionario saranno i nomi dei fogli e i valori il dataframe invece\n",
    "\n",
    "print(type(all_sheets))\n",
    "\n",
    "for key, value in all_sheets.items():\n",
    "    print(key,type(value), value.shape)\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unire tutti i fogli di un file excel in un solo dataframe (presupponendo che abbiano le stesse colonne)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4000, 11)\n",
      "['uk-500' 'Foglio2']\n"
     ]
    }
   ],
   "source": [
    "# Creare un df vuoto\n",
    "merge_sheets = pd.DataFrame()\n",
    "\n",
    "for sheet, frame in all_sheets.items():\n",
    "    # possiamo aggiungere una colonna al df per tenere traccia della provenienza del dato\n",
    "    frame[\"sheet_name\"] = sheet\n",
    "    # tramite il metodo append possiamo aggiungere il dataframe del dizionario ad un df vuoto\n",
    "    merge_sheets = merge_sheets.append(frame)\n",
    "    \n",
    "print(merge_sheets.shape)\n",
    "\n",
    "# controllare che siano presenti tutti i fogli con il metodo \"unique\" (select distinct) sulla colonna dei nomi\n",
    "print(merge_sheets[\"sheet_name\"].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h6> Modifying imports: true/false data </h6>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AttendedBootcamp     float64\n",
      "BootcampFinish       float64\n",
      "BootcampLoanYesNo    float64\n",
      "ID.x                  object\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "data_path = \"Data/fcc-new-coder-survey.xlsx\"\n",
    "columns = [\"ID.x\",\"BootcampLoanYesNo\",\"AttendedBootcamp\",\"BootcampFinish\"]\n",
    "boolean_df = pd.read_excel(data_path,usecols=columns, skiprows=2)\n",
    "\n",
    "boolean_df.head()\n",
    "# Le colonne non vengono interpretate come booleane\n",
    "print(boolean_df.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AttendedBootcamp                                                  37.0\n",
      "BootcampFinish                                                    21.0\n",
      "BootcampLoanYesNo                                                 14.0\n",
      "ID.x                 cef35615d61b202f1dc794ef2746df14323e5a113644d1...\n",
      "dtype: object\n",
      "AttendedBootcamp       6\n",
      "BootcampFinish       965\n",
      "BootcampLoanYesNo    964\n",
      "ID.x                   0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# esplorazione\n",
    "# conta delle colonne che hanno come valore vero (1)\n",
    "print(boolean_df.sum())\n",
    "# conta degli elementi nulli nelle colonne\n",
    "print(boolean_df.isna().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rilettura del df specificando il tipo delle colonne\n",
    "#dct_type = {\"AttendedBootcamp\":bool,\"BootcampFinish\":bool,\"BootcampLoanYesNo\":bool,\"ID.x\":str}\n",
    "#boolean_df = pd.read_excel(data_path, usecols=columns, skiprows=2,dtype=dct_type)\n",
    "\n",
    "# Attenzione ai valori NA!\n",
    "\n",
    "# Per le colonne \"custom\" (per esempio dove sono presenti dei valori Sì/No) è possibile specificare cosa\n",
    "# viene considerato False utilizzando il parametro \"false_values\" per settare i valori false custom, \n",
    "# analogamente è possibile utilizzare true_values\n",
    "\n",
    " #survey_subset = pd.read_excel(\"fcc_survey_yn_data.xlsx\",\n",
    "  #                            dtype={\"HasDebt\": bool,\n",
    "   #                           \"AttendedBootCampYesNo\": bool},\n",
    "    #                          false_values=[\"No\"],\n",
    "     #                         true_values=[\"Yes\"])\n",
    "\n",
    "# cosa fare per quanto riguarda gli na dipende dal tipo dei dati"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h6>Modifying imports: parsing dates</h6>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ID.x                      object\n",
      "Part1EndTime      datetime64[ns]\n",
      "Part1StartTime    datetime64[ns]\n",
      "Part2EndTime      datetime64[ns]\n",
      "Part2StartTime    datetime64[ns]\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "# pandas archivia di default le date come oggetti, per salvare come date i valori presenti in una colonna\n",
    "# si utilizza il parametro \"parse_dates\", questo accetta: una lista di colonne, una lista di liste che devono\n",
    "# essere combinate assieme, oppure un dizionatio\n",
    "\n",
    "date_cols = [\"Part1EndTime\",\"Part1StartTime\",\"Part2EndTime\",\"Part2StartTime\"]\n",
    "\n",
    "#esempio in caso di colonne da unire, le colonne nella sottolista verranno unite in un unico datetime\n",
    "#date_cols = [\"Part1EndTime\",\"Part1StartTime\",[\"Part2EndDate\",\"Part2EndTime\"]]\n",
    "\n",
    "# parse_dates funziona soltante se le date sono in formato standard\n",
    "data_path = \"Data/fcc-new-coder-survey.xlsx\"\n",
    "columns = [\"ID.x\",\"Part1EndTime\",\"Part1StartTime\",\"Part2EndTime\",\"Part2StartTime\"]\n",
    "boolean_df = pd.read_excel(data_path,usecols=columns, skiprows=2, parse_dates=date_cols)\n",
    "\n",
    "boolean_df.head()\n",
    "# Le colonne non vengono interpretate come booleane\n",
    "print(boolean_df.dtypes)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "colonne con date non standard non funzionano con parse_dates\n",
    "possono essere convertite utilizzando il metodo pandas \"to_datetime\", argomenti:\n",
    "* df e colonna da convertire\n",
    "* porametro \"format\" che descriva il formato della data da convertire"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# es. di formato\n",
    "\n",
    "format_string = \"%m%d%Y %H:%M:%S\"\n",
    "\n",
    "#example_df[\"datetime_column\"] = pd.to_datetime(survey_df[\"datetime_column\"], format=format_string)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> 3 - Importing Data from Databases </h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h6> Introduction to databases </h6>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: sqlalchemy in /home/marasga/.local/lib/python3.9/site-packages (1.3.23)\n"
     ]
    }
   ],
   "source": [
    "# SQLLite db sono archiviati come file\n",
    "# la libreria per utilizzare i db è sqlalchemy\n",
    "!pip3 install sqlalchemy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlalchemy as sql\n",
    "# FROM SQLALCHEMY import create_engine"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Per connettersi ad un db bisogna effettuare le operazioni:\n",
    "* creare un'istanza db engine attraverso il metodo \"create_engine()\" di sqlalchemy\n",
    "* questo metodo richiede la stringa di connessione e il tipo di engine e.g. sqlite:///filename.db"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Una volta connessi al db è possibile utilizzare la funzione \"read_sql\" di pandas per effetture delle query sul db, la funzione riceve principalmente due argomenti <b>pd.read_sql(query,engine)</b>:\n",
    "* query: la stringa chge contiene la query sql da eseguire\n",
    "* engine: connesione/oggetto db engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = \"Data/data.db\"\n",
    "\n",
    "engine = sql.create_engine(\"sqlite:///\" + data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "station       object\n",
      "name          object\n",
      "latitude     float64\n",
      "longitude    float64\n",
      "elevation    float64\n",
      "date          object\n",
      "month         object\n",
      "awnd         float64\n",
      "prcp         float64\n",
      "snow         float64\n",
      "tavg          object\n",
      "tmax           int64\n",
      "tmin           int64\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "test = pd.read_sql(\"SELECT * FROM weather;\",engine)\n",
    "#in alternativa per selezionare tutti i dati della tabella basta specificarne il nome nella stringa\n",
    "test.head(5)\n",
    "print(test.dtypes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> 3 - Importing JSON Data and Working with APIs </h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Per leggere dati json utilizzare la funzione <b>read_json()</b>, gli argomenti che accetta sono:\n",
    "* una stringa come path ad un json o un json in formato stringa\n",
    "* <b>dtype</b> per specificare il tipo dei dati\n",
    "* <b>orient</b> per flaggare data layout di json non comuni\n",
    "\n",
    "I dati json <b>non sono</b> tabulari, pandas cerca di inserirli in forma tabellare\n",
    "Nei json l'orientazione in colonna è generalmente più efficente in termini di spazio.\n",
    "Sono possibili altri tipi di orientazione come la split-orientation per la quale colonne, indici e dati sono inseriti in array separati all'interno di un json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pandas could not parse the JSON.\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    # Load the JSON with orient specified\n",
    "    df = pd.read_json(\"dhs_report_reformatted.json\",\n",
    "                      orient=\"split\")\n",
    "    \n",
    "    # Plot total population in shelters over time\n",
    "    df[\"date_of_census\"] = pd.to_datetime(df[\"date_of_census\"])\n",
    "    df.plot(x=\"date_of_census\", \n",
    "            y=\"total_individuals_in_shelter\")\n",
    "    plt.show()\n",
    "    \n",
    "except ValueError:\n",
    "    print(\"pandas could not parse the JSON.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h6> Introduction to APIs</h6>\n",
    "\n",
    "Una libreria che può essere utilizzata per effettuare richieste http è requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: requests in /usr/lib/python3.9/site-packages (2.24.0)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/lib/python3.9/site-packages (from requests) (3.0.4)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /usr/lib/python3.9/site-packages (from requests) (2.10)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/lib/python3.9/site-packages (from requests) (1.25.8)\n"
     ]
    }
   ],
   "source": [
    "!pip3 install requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# La funzione predefinita per l'interrogazione di un API è get(), prendere come argomento l'url\n",
    "# il secondo argomento è params, un dizionario dei parametri da passare per la richiesta\n",
    "# il terzo è heades, un dizionario che viene utilizzato per l'autenticazione\n",
    "\n",
    "import requests\n",
    "\n",
    "url_string=\"https://jsonplaceholder.typicode.com/todos/1\"\n",
    "\n",
    "# il risultato della richiesta è un oggetto response\n",
    "\n",
    "response = requests.get(url_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'userId': 1, 'id': 1, 'title': 'delectus aut autem', 'completed': False}\n"
     ]
    }
   ],
   "source": [
    "# l'oggetto response contiene dati e metadati, il metodo \"json()\" permette di ottenere solamente i dati\n",
    "print(response.json())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Per caricare i dati ottenuti da una richiesta ad un api non è possibile utilizzare il metodo pandas <b>read_json</b> infatti questo si aspetta una stringa e non un dizionario, dobbiamo utilizzare il costruttore <b>DataFrame</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'error': {'code': 'UNAUTHORIZED_ACCESS_TOKEN', 'description': 'The access token provided is not currently able to query this endpoint.'}}\n"
     ]
    }
   ],
   "source": [
    "api_url = \"https://api.yelp.com/v3/businesses/search\"\n",
    "headers = {'Authorization': 'Bearer mhmt6jn3SFPVC1u6pfwgHWQvsa1wmWvCpKRtFGRYlo4mzA14SisQiDjyygsGMV2Dm7tEsuwdC4TYSA0Ai_GQTjKf9d5s5XLSNfQqdg1oy7jcBBh1i7iQUZBujdA_XHYx'}\n",
    "params = {'term': 'cafe', 'location': 'NYC'}\n",
    "# Get data about NYC cafes from the Yelp API\n",
    "response = requests.get(api_url, \n",
    "                headers=headers, \n",
    "                params=params)\n",
    "\n",
    "# Extract JSON data from the response\n",
    "data = response.json()\n",
    "print(data)\n",
    "\n",
    "# Load data to a data frame\n",
    "#cafes = pd.DataFrame(data[\"businesses\"])\n",
    "\n",
    "# View the data's dtypes\n",
    "#print(cafes.dtypes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h6>Working with nested JSONs</h6>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pandas contiene delle funzioni per appiattire i json annidati. C'è un submodulo specifico <b> pandas.io.json </b> questo deve essere importato in maniera esplicita,\n",
    "il metodo <b>json_normalize</b>:\n",
    "* riceve come argomenti un dizionario o una lista di dizionari\n",
    "* returna un df dei dati tabulare\n",
    "* il name pattern è del tipo attribute.nestedone\n",
    "* riceve un argomento <b>sep</b> per definire il separatore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load json_normalize()\n",
    "from pandas.io.json import json_normalize\n",
    "\n",
    "# Isolate the JSON data from the API response\n",
    "data = response.json()\n",
    "\n",
    "# Flatten business data into a data frame, replace separator\n",
    "cafes = json_normalize(data[\"businesses\"],\n",
    "             sep=\"_\")\n",
    "\n",
    "# View data\n",
    "print(cafes.head())\n",
    "\n",
    "# Load other business attributes and set meta prefix\n",
    "flat_cafes = json_normalize(data[\"businesses\"],\n",
    "                            sep=\"_\",\n",
    "                    \t\trecord_path=\"categories\",\n",
    "                    \t\tmeta=[\"name\", \n",
    "                                  \"alias\",  \n",
    "                                  \"rating\",\n",
    "                          \t\t  [\"coordinates\", \"latitude\"], \n",
    "                          \t\t  [\"coordinates\", \"longitude\"]],\n",
    "                    \t\tmeta_prefix=\"biz_\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# View the data\n",
    "print(flat_cafes.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h6>Combining multiple datasets</h6>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Append:\n",
    "Il metodo <b>append</b> permette di aggiungere le righe di un df in un altro, per rinumerare le righe utilizzando l'index del df di destinazione è necessario settare <b>ignore_index</b> = True\n",
    "2. Merge:\n",
    "Il metodo <b>merge</b> svolge lo stesso compito di SQL JOIN combinando i dataset per aggiungere colonner legate da una relazione. E' sia una funzione di pandas che un metodo df.\n",
    "df.merge riceve come argomenti:\n",
    "    * Il secondo df\n",
    "    * <b>on</b> se i nomi delle colonne sono gli stessi in entrambi i df\n",
    "    * <b>left_on</b> e <b>right_on</b> se i nomi sono differenti\n",
    "    * NB: Le colonne devono essere dello stesso datatype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'json_normalize' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-36-a7192da77500>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrequests\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mapi_url\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheaders\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mheaders\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mnext_50_cafes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjson_normalize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjson\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"businesses\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;31m# Append the results, setting ignore_index to renumber rows\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'json_normalize' is not defined"
     ]
    }
   ],
   "source": [
    "# Add an offset parameter to get cafes 51-100\n",
    "params = {\"term\": \"cafe\", \n",
    "          \"location\": \"NYC\",\n",
    "          \"sort_by\": \"rating\", \n",
    "          \"limit\": 50,\n",
    "          \"offset\":50}\n",
    "\n",
    "result = requests.get(api_url, headers=headers, params=params)\n",
    "next_50_cafes = json_normalize(result.json()[\"businesses\"])\n",
    "\n",
    "# Append the results, setting ignore_index to renumber rows\n",
    "cafes = top_50_cafes.append(next_50_cafes,ignore_index=True)\n",
    "\n",
    "# Print shape of cafes\n",
    "print(cafes.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge crosswalk into cafes on their zip code fields\n",
    "cafes_with_pumas = cafes.merge(crosswalk, left_on=\"location_zip_code\",right_on=\"zipcode\")\n",
    "\n",
    "\n",
    "\n",
    "# Merge pop_data into cafes_with_pumas on puma field\n",
    "cafes_with_pop = cafes_with_pumas.merge(pop_data, on=\"puma\")\n",
    "\n",
    "# View the data\n",
    "print(cafes_with_pop.head())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
